From GPU-Poor to GPU-Rich: What You Can Run at 24 GB, 96 GB, and 100 GB+

How much does it cost to run local LLMs? Which models can you run locally?

Discrete NVIDIA GPUs vs unified memory

Ground rules:
- Need to run models at decent speed - 40-50 tokens / second and above. For live LLM inferencing, not batch. So no offloading to CPU.
