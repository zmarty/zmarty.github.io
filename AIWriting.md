# Signs of AI Writing

This is a list of writing and formatting conventions typical of AI chatbots such as ChatGPT, with real examples. It is a field guide to help detect undisclosed AI-generated content. Not all text featuring these indicators is AI-generated, as the large language models that power AI chatbots are trained on human writing.

Moreover, this list is **descriptive**, not **prescriptive**; it consists of observations, not rules.

The patterns here are also only potential **signs** of a problem, not **the problem itself**. While many of these issues are immediately obvious and easy to fix—e.g., excessive boldface, broken markup, citation style quirks—they can point to less outwardly visible problems that carry much more serious risks.

## Caveats

### AI Detection Tools

Do not solely rely on artificial intelligence content detection tools (such as GPTZero). While they perform better than random chance, these tools have non-trivial error rates. Detectors can be susceptible to factors such as text modifications (e.g. paraphrasing and spacing changes) and the use of models not seen during detector training.

### Your Detection Ability

Do not rely too much on your own judgment. While research on humans' abilities to detect AI-generated text is limited, a 2025 preprint shows that heavy users of LLMs can correctly determine whether an article was generated by AI about 90% of the time, which means that if you are an expert user of LLMs and you tag 10 pages as being AI-generated, you've probably falsely accused one editor. People who don't use LLMs much do only slightly better than random chance (in both directions).

## Content

LLMs (and artificial neural networks in general) use statistical algorithms to guess (infer) what should come next based on a large corpus of training material. It thus tends to regress to the mean; that is, the result tends toward the most statistically likely result that applies to the widest variety of cases. It can simultaneously be a strength and a "tell" for detecting AI-generated content.

For example, LLMs are usually trained on data from the internet in which famous people are generally described with positive, important-sounding language. Consequently, the LLM tends to omit specific, unusual, nuanced facts (which are statistically rare) and replace them with more generic, positive descriptions (which are statistically common). Thus the highly specific "inventor of the first train-coupling device" might become "a revolutionary titan of industry." It is like shouting louder and louder that a portrait shows a uniquely important person, while the portrait itself is fading from a sharp photograph into a blurry, generic sketch. The subject becomes simultaneously less specific and more exaggerated.

This statistical regression to the mean, a smoothing over of specific facts into generic statements, that could equally apply to many topics, makes AI-generated content easier to detect.

Moreover, each model and version of AI chatbots have a distinctive way of writing (idiolect), so that what is typical for ChatGPT-4 might not be characteristic to Gemini.

### Undue Emphasis on Significance, Legacy, and Broader Trends

**Words to watch:** *stands/serves as*, *is a testament/reminder*, *a vital/significant/crucial/pivotal/key role/moment*, *underscores/highlights its importance/significance*, *reflects broader*, *symbolizing its ongoing/enduring/lasting*, *contributing to the*, *setting the stage for*, *marking/shaping the*, *represents/marks a shift*, *key turning point*, *evolving landscape*, *focal point*, *indelible mark*, *deeply rooted*, ...

LLM writing often puffs up the importance of the subject matter by adding statements about how arbitrary aspects of the topic represent or contribute to a broader topic. There is a distinct and easily identifiable repertoire of ways that it writes these statements.

> The Statistical Institute of Catalonia was officially established in 1989, **marking a pivotal moment** in the evolution of regional statistics in Spain. [...]
>
> The founding of Idescat **represented a significant shift** toward regional statistical independence, enabling Catalonia to develop a statistical system tailored to its unique socio-economic context. This initiative **was part of a broader movement** across Spain to decentralize administrative functions and enhance regional governance.

LLMs may include these statements for even the most mundane of subjects like etymology or population data. Sometimes, they add hedging preambles acknowledging that the subject is relatively unimportant or low-profile, before talking about its importance anyway.

When talking about biology (e.g., when asked to discuss an animal or plant species), LLMs tend to over-emphasize connections to the broader ecosystem or environment, even when those connections are tenuous or generic. LLMs also tend to belabor the species' conservation status and research and preservation efforts, even if the status is unknown and no serious efforts exist.

### Undue Emphasis on Notability, Attribution, and Media Coverage

**Words to watch:** *independent coverage*, *local/regional/national/[country name] media outlets*, *music/business/tech outlets*, *profiled in*, *written by a leading expert*, *active social media presence*

Similarly, LLMs act as if the best way to prove that a subject is notable is to hit readers over the head with claims of notability, often by listing sources that a subject has been covered in. They may or may not provide additional context as to what those sources have actually said about the subject, and often inaccurately attribute their own superficial analyses to the source. This is more common in text from newer AI tools (2025 or later).

In articles about people/entities who use social media, LLMs will often note that they "maintain an active social media presence" or something similar. This wording is particularly idiosyncratic to AI text.

In some cases, LLMs will create entire sections to assert notability, with a breakdown of the sources that have covered the topic in a list format.

### Superficial Analyses

**Words to watch:** *highlighting/underscoring/emphasizing ...*, *ensuring ...*, *reflecting/symbolizing ...*, *contributing to ...*, *cultivating/fostering ...* (in the figurative sense), *encompassing ...*, *valuable insights*, *align/resonate with*

AI chatbots tend to insert superficial analysis of information, often in relation to its significance, recognition, or impact. This is often done by attaching a present participle ("-ing") phrase at the end of sentences, sometimes with vague attributions to third parties.

> As of the April 2008 census, the population of Douera stood at approximately 56,998 inhabitants, **creating a lively community within its borders.** Situated in the central-north region of the country, Douera enjoys close proximity to the capital city, Algiers, **further enhancing its significance as a dynamic hub of activity and culture.** With its coastal charm and convenient location, Douera captivates both residents and visitors alike, **offering a diverse range of experiences against the backdrop of Algeria's stunning natural beauty.**

### Promotional and Advertisement-like Language

**Words to watch:** *boasts a*, *vibrant*, *rich* (in the figurative sense), *profound*, *enhancing its*, *showcasing*, *exemplifies*, *commitment to*, *natural beauty*, *nestled*, *in the heart of*, *groundbreaking* (in the figurative sense), *renowned*, ...

LLMs have serious problems keeping a neutral tone, especially when writing about something that could be considered "cultural heritage"—in which case they constantly remind the reader of its importance. This happens even when LLMs are prompted to use an encyclopedic tone; they may insert promotional language even while claiming to remove it.

> **Nestled** within the **breathtaking** region of Gonder in Ethiopia, Alamata Raya Kobo **stands as a vibrant town with a rich cultural heritage and a significant place** within the Amhara region. **From its scenic landscapes to its historical landmarks**, Alamata Raya Kobo **offers visitors a fascinating glimpse into the diverse tapestry** of Ethiopia.

In a similar way, LLM chatbots also add promotional/positive-sounding language to text about companies, business, and products, such that it sounds more like the transcript of a TV commercial.

### Vague Attributions and Overgeneralization of Opinions

**Words to watch:** *Industry reports*, *Observers have cited*, *Experts argue*, *Some critics argue*, *several sources/publications* (when only few sources are cited), *such as* (before exhaustive word lists), ...

AI chatbots tend to attribute opinions or claims to some vague authority—a practice called weasel wording.

> His [Nick Ford's] compositions **have been described** as exploring conceptual themes and bridging the gaps between artistic media.

Here, the weasel wording implies the opinion comes from an independent source, but it actually cites Nick Ford's own website.

AI chatbots also commonly exaggerate the quantity of sources that these opinions are attributed to. They may present views from one or two sources as widely held (often combined with the vague attributions above), mention the existence or opinion of multiple "reviewers" or "scholars" while only citing one person, or imply that lists of examples are non-exhaustive when the sources give no indication that other examples exist.

### Outline-like Conclusions About Challenges and Future Prospects

**Words to watch:** *Despite its... faces several challenges...*, *Despite these challenges*, *Challenges and Legacy*, *Future Outlook* ...

Many LLM-generated articles include a "Challenges" section, which typically begins with a sentence like "Despite its [positive/promotional words], [article subject] faces challenges..." and ends with either a vaguely positive assessment of the article subject, or speculation about how ongoing or potential initiatives could benefit the subject.

> **Despite its industrial and residential prosperity, Korattur faces challenges** typical of urban areas, including[...] With its **strategic location and ongoing initiatives**, Korattur **continues to thrive** as an integral part of the Ambattur industrial zone, embodying the synergy between industry and residential living.

### Leads Treating Lists or Broad Article Titles as Proper Nouns

In AI-generated articles about topics with a title that is not a proper name, such as a list, the first sentence of the lead may introduce and/or define the article's title as if it were a standalone real-world entity.

> **"The Effects of Foreign language anxiety on Learning" refers to** the feelings of tension, nervousness, and apprehension experienced when learning or using a language other than one's native tongue.

### Vague See Also Sections

LLMs tend to fill "see also" sections with broad terms. For example, a see also section on an article about a startup may link to Financial technology. Such entries may link to non-existent articles or be unlinked altogether.

## Language and Grammar

### Overused "AI Vocabulary" Words

**Words to watch:** *Additionally,* (especially beginning a sentence), *align with*, *crucial*, *delve* (pre-2025), *emphasizing*, *enduring*, *enhance*, *fostering*, *garner*, *highlight* (as a verb), *interplay*, *intricate/intricacies*, *key* (as an adjective), *landscape* (as an abstract noun), *pivotal*, *showcase*, *tapestry* (as an abstract noun), *testament*, *underscore* (as a verb), *valuable*, *vibrant*

Many studies have demonstrated that LLMs overuse specific words. These words started appearing *far* more frequently in text after 2023 than they did in comparable text from before 2023, which is almost certain to be human-written as it predates widespread LLM use. They often co-occur in LLM output: where there is one, there are likely others. One or two of these words appearing in an edit may be coincidental, but an edit (post-2022) introducing lots of them, lots of times, is one of the strongest tells for AI use.

The distribution of "AI vocabulary" is slightly different depending on which chatbot or LLM was used, and has changed over time. For instance, the word *delve* was famously overused by ChatGPT in 2023 and early 2024, but became less frequent later in 2024, then dropped off sharply in 2025.

Please keep context in mind. For example, while the word "underscore" is overused in (pre-GPT-5) AI text, it can also refer to a literal underline mark or to incidental music.

### Avoidance of Basic Copulatives ("is"/"are" phrases)

**Words to watch:** *serves as/stands as/marks/represents [a]*, *boasts/features/offers [a]*

LLM-generated text often substitutes constructions like *serves as a* or *mark the* for their simpler counterparts that use copulas such as *is* or *are*. One study documented an over 10% decrease in the usage of the words *is* and *are* in academic writing in 2023, with no major changes in their frequency before that. Similarly, it prefers phrases with *features*, *offers*, and the like to their more neutral counterparts with *has*.

This is particularly visible in AI copyedits, which will often "improve" text in this way.

### Negative Parallelisms

Parallel constructions involving "not", "but", or "however" such as "Not only ... but ..." or "It is not just about ..., it's ..." are common in LLM writing in order to appear balanced and thoughtful.

> **Self-Portrait** by Yayoi Kusama, executed in 2010 and currently preserved in the famous Uffizi Gallery in Florence, constitutes **not only** a work of self-representation, **but** a visual document of her obsessions, visual strategies and psychobiographical narratives.

Asides from parallelisms that merely direct readers' attention towards secondary properties, there have also been constructions that explicitly negate primary properties altogether. These are often expressed along the lines of "not ..., it's ..." or "no ..., no ..., just ...".

### Rule of Three

LLMs overuse the "rule of three." This can take different forms, from "adjective, adjective, adjective" to "short phrase, short phrase, and short phrase." LLMs often use this structure to make superficial analyses appear more comprehensive.

> The Amaze Conference brings together **global SEO professionals, marketing experts, and growth hackers** to discuss the latest trends in digital marketing. The event features **keynote sessions, panel discussions, and networking opportunities**.

### Elegant Variation

Generative AI has a repetition-penalty code, meant to discourage it from reusing words too often. For instance, the output might give a main character's name and then repeatedly use a different synonym or related term (e.g., protagonist, key player, eponymous character) when mentioning it again.

### False Ranges

When "from ... to ..." constructions are not used figuratively, they are used to indicate the lower and upper bounds of a scale. LLMs really like mixing it up, such as when giving examples of items within a set. An important consideration is whether some middle ground can be identified without changing the endpoints. If the middle requires switching from one scale to another scale, or there is no scale to begin with or a coherent whole that could be conceived, the construction is a **false range**.

> Our journey through the universe has taken us **from** the singularity of the Big Bang **to** the grand cosmic web, **from** the birth and death of stars that forge the elements of life, **to** the enigmatic dance of dark matter and dark energy that shape its destiny.

## Style

### Title Case

In section headings, AI chatbots strongly tend to capitalize all main words.

### Overuse of Boldface

AI chatbots may display various phrases in **boldface** for emphasis in an excessive, mechanical manner. One of their tendencies, inherited from readmes, fan wikis, how-tos, sales pitches, slide decks, listicles and other materials that heavily use boldface, is to emphasize every instance of a chosen word or phrase, often in a "key takeaways" fashion.

> It blends **OKRs (Objectives and Key Results)**, **KPIs (Key Performance Indicators)**, and visual strategy tools such as the **Business Model Canvas (BMC)** and **Balanced Scorecard (BSC)**. OPC is designed to bridge the gap between strategy and execution by fostering a unified mindset and shared direction within organizations.

### Inline-Header Vertical Lists

AI chatbots output often includes vertical lists formatted in a specific way: an ordered or unordered list where the list marker (number, bullet, dash, etc.) is followed by an inline boldfaced header, separated with a colon from the remaining descriptive text.

> AVO consists of three key layers:
> * **SEO (Search Engine Optimization):** Traditional methods for improving visibility in search engine results through content, technical, and on-page optimization.
> * **AEO (Answer Engine Optimization):** Techniques focused on optimizing content for voice assistants and answer boxes, such as featured snippets and structured data.
> * **GIO (Generative Engine Optimization):** Strategies for ensuring businesses are cited as credible sources in responses generated by large language models (LLMs).

### Emoji

AI chatbots often use emoji. In particular, they sometimes decorate section headings or bullet points by placing emoji in front of them. This is most noticeable in talkpage comments.

### Overuse of Em Dashes

While human editors and writers often use em dashes (—), LLM output uses them more often than nonprofessional human-written text of the same genre, and uses them in places where humans are more likely to use commas, parentheses, colons, or (misused) hyphens (-). LLMs especially tend to use em dashes in a formulaic, pat way, often mimicking "punched up" sales-like writing by over-emphasizing clauses or parallelisms.

This sign is most useful when taken in combination with other indicators, not by itself.

### Unusual Use of Tables

AIs tend to create unnecessary small tables that could be better represented as prose.

### Curly Quotation Marks and Apostrophes

ChatGPT and DeepSeek typically use curly quotation marks ("..." or '...') instead of straight quotation marks ("..." or '...'). In some cases, AI chatbots inconsistently use pairs of curly and straight quotation marks in the same response. They also tend to use the curly apostrophe ('), the same character as the curly right single quotation mark, instead of the straight apostrophe (').

Curly quotes alone do not prove LLM use. Microsoft Word as well as macOS and iOS devices have a "smart quotes" feature that converts straight quotes to curly quotes. Additionally, Gemini and Claude models typically do not use curly quotes.

### Subject Lines

User messages generated by AI chatbots sometimes begin with text that is intended to be pasted into the Subject field on an email form.

> Subject: Request for Permission to Edit Wikipedia Article - "Dog"

## Communication Intended for the User

### Collaborative Communication

**Words to watch:** *I hope this helps*, *Of course!*, *Certainly!*, *You're absolutely right!*, *Would you like...*, *is there anything else*, *let me know*, *more detailed breakdown*, *here is a* ...

Editors sometimes paste text from an AI chatbot that was meant as correspondence, prewriting or advice, rather than article content.

> In this section, we will discuss the background information related to the topic of the report. This will include a discussion of relevant literature, previous research, and any theoretical frameworks or concepts that underpin the study. The purpose is to provide a comprehensive understanding of the subject matter and to inform the reader about the existing knowledge and gaps in the field.

### Knowledge-Cutoff Disclaimers and Speculation About Gaps in Sources

**Words to watch:** *as of [date]*, *Up to my last training update*, *as of my last knowledge update*, *While specific details are limited/scarce...*, *not widely available/documented/disclosed*, *...in the provided/available sources/search results...*, *based on available information* ...

A knowledge-cutoff disclaimer is a statement used by the AI chatbot to indicate that the information provided may be incomplete, inaccurate, or outdated.

> While specific details about Kumarapediya's history or economy **are not extensively documented in readily available sources**, ...

> **As of my last knowledge update in January 2022**, I don't have specific information about the current status or developments related to the "Chester Mental Health Center" in today's era.

### Phrasal Templates and Placeholder Text

AI chatbots may generate responses with fill-in-the-blank phrasal templates for the LLM user to replace with words and phrases pertaining to their use case. However, some LLM users forget to fill in those blanks.

> Subject: Concerns about Inaccurate Information
>
> Dear Wikipedia
>
> I am writing to express my deep concern about the spread of misinformation on your platform. Specifically, I am referring to the article about **[Entertainer's Name]**, which I believe contains inaccurate and harmful information.

Large language models may also insert placeholder dates like "2025-xx-xx" into citation fields.

## Markup

### Use of Markdown

A lot of AI chatbots are not proficient in wikitext, the markup language used to instruct Wikipedia's MediaWiki software how to format an article. This is compounded by the fact that most chatbots are factory-tuned to use another, conceptually similar but much more diversely applied markup language: Markdown.

Markdown syntax is completely different from wikitext. Markdown uses asterisks (*) or underscores (_) instead of single-quotes (') for bold and italic formatting, hash symbols (#) instead of equals signs (=) for section headings, parentheses (()) instead of square brackets ([]) around URLs, and three symbols (---, ***, or ___) instead of four hyphens (----) for thematic breaks.

The presence of faulty wikitext syntax mixed with Markdown syntax is a strong indicator that content is LLM-generated. However, Markdown *alone* is not such a strong indicator. Software developers, researchers, technical writers, and experienced internet users frequently use Markdown in tools like Obsidian and GitHub.

### Broken Wikitext

Since AI chatbots are typically not proficient in wikitext and templates, they often produce faulty syntax.

### turn0search0

ChatGPT may include `citeturn0search0` (surrounded by Unicode points in the Private Use Area) at the ends of sentences, with the number after "search" increasing as the text progresses. These are places where the chatbot links to an external site, but a human pasting the conversation has that link converted into placeholder code. This was first observed in February 2025.

### Reference Markup Bugs: contentReference, oaicite, oai_citation, +1, attached_file, grok_card

Due to a bug, ChatGPT may add code in the form of `:contentReference[oaicite:0]{index=0}`, `Example+1`, or `oai_citation` in place of links to references in output text.

As of fall 2025, tags like `[attached_file:1]`, `[web:1]` have been seen at the end of sentences. This may be Perplexity-specific.

Though Grok-generated text is rare compared to other chatbots, it may sometimes include XML-styled *grok_card* tags after citations.

### attribution and attributableIndex

ChatGPT may add JSON-formatted code at the end of sentences in the form of `({"attribution":{"attributableIndex":"X-Y"}})`, with X and Y being increasing numeric indices.

### Non-Existent or Out-of-Place Categories

LLMs may hallucinate non-existent categories, sometimes for generic concepts that *seem like* plausible category titles (or SEO keywords), and sometimes because their training set includes obsolete and renamed categories.

### Non-Existent Templates

LLMs often hallucinate non-existent templates (especially plausible-sounding types of infoboxes) and template parameters. LLMs may also use templates that were deleted after their knowledge cutoff date.

## Citations

### Broken External Links

If a new article or draft has multiple citations with external links, and several of them are broken (e.g., returning 404 errors), this is a strong sign of an AI-generated page, particularly if the dead links are not found in website archiving sites like Internet Archive or Archive Today.

### Invalid DOI and ISBNs

A checksum can be used to verify ISBNs. An invalid checksum is a very likely sign that an ISBN is incorrect. Similarly, DOIs are more resistant to link rot than regular hyperlinks. Unresolvable DOIs and invalid ISBNs can be indicators of hallucinated references.

### Outdated Access Dates

In some AI-assisted text, citations may include an *access-date* by default, but the date can look unexpectedly old relative to when the edit was made. If a large number of citations share the same old *access-date* this can be a sign of AI-assisted text.

### DOIs That Lead to Unrelated Articles

A LLM may generate references to non-existent scholarly articles with DOIs that appear valid but are, in reality, assigned to unrelated articles.

### Book Citations Without Page Numbers or URLs

LLMs often generate book citations that do not include page numbers. Without the page number, that citation is not useful for verifying the claims in the prose.

Some LLM-generated book citations include page numbers, and the book exists, but the cited pages do not verify the text.

### Incorrect or Unconventional Use of References

AI tools may have been prompted to include references, and make an attempt to do so as expected, but fail with some key implementation details or stand out when compared with conventions.

### utm_source= Parameter

ChatGPT may add the UTM parameters `utm_source=openai` or `utm_source=chatgpt.com` to URLs that it is using as sources. Microsoft Copilot may add `utm_source=copilot.com` to URLs. Grok uses `referrer=grok.com`. Other LLMs, such as Gemini or Claude, use UTM parameters less often.

Note: While this does definitively prove ChatGPT's involvement, it doesn't prove, on its own, that ChatGPT also generated the writing. Some editors use AI tools to find citations for existing text.

### Named References Declared But Unused

LLM-generated content may declare named references in a references section but never actually use them in the article body.

## Miscellaneous

### Sudden Shift in Writing Style

A sudden shift in an editor's writing style, such as unexpectedly flawless grammar compared to their other communication, may indicate the use of AI tools. A mismatch of user location, national ties of the topic to a variety of English, and the variety of English used may indicate the use of AI tools. LLM outputs use American English by default, unless prompted otherwise.

### Overwhelmingly Verbose Edit Summaries

AI-generated edit summaries are often unusually long, written as formal, first-person paragraphs without abbreviations, and/or conspicuously itemize conventions.

> ChatGPT I revised the content to provide a neutral and informative description of the Indira Gandhi National Centre for the Arts (IGNCA). The focus was on presenting the institution's objectives, approach, and programs in a way that adheres to Wikipedia's guidelines. The tone was adjusted to be more encyclopedic and less promotional.

### "Submission Statements" in Drafts

At least one LLM tends to insert "submission statements" supposedly intended for reviewers that supposedly explain why the subject is notable and why the draft meets guidelines. Of course, all this actually does is let reviewers know that the draft is LLM-generated.

### Pre-Placed Maintenance Templates

Occasionally a new editor creates a draft that includes a review template already set to "declined." LLMs have been known to create pages that already have maintenance templates that shouldn't plausibly be there.

## Signs of Human Writing

### Age of Text Relative to ChatGPT Launch

ChatGPT was launched to the public on November 30, 2022. Although OpenAI had similarly powerful LLMs before then, they were paid services and not easily accessible or known to lay people. Thus, it is very unlikely that any particular text added **before November 30, 2022** was generated by an LLM.

### Ability to Explain One's Own Editorial Choices

Editors should be able to explain why they made an edit or mistake. For example, if an editor inserts a URL that appears fabricated, you can ask how the mix-up occurred instead of jumping to conclusions. If they can supply the correct link and explain it as a human error, that points to an ordinary human error.

## Ineffective Indicators

False accusations of AI use can drive away new editors and foster an atmosphere of suspicion. Before claiming AI was used, consider if Dunning–Kruger effect and confirmation bias is clouding your judgement. Here are several somewhat commonly used indicators that are ineffective in LLM detection—and may even indicate the opposite.

* **Perfect grammar**: While modern LLMs are known for their high grammatical proficiency, many editors are also skilled writers or come from professional writing backgrounds.

* **Combination of casual and formal registers, or language that sounds both "clinical" and "emotional"**: This may indicate the casual writing of a person in a technical field, such as computer science. It may also indicate youth, a preference for mixed registers, playfulness, or neurodivergence.

* **"Bland" or "robotic" prose**: By default, modern LLMs tend toward effusive and verbose prose, as detailed above; while this tendency is formulaic, it may not scan as "robotic" to those unfamiliar with AI writing.

* **"Fancy," "academic," or unusual words**: While LLMs disproportionately favor certain words and phrases, many of which are long and have difficult readability scores, the correlation does not extend to *all* "fancy," academic, or "advanced"-sounding prose. Low-frequency and "unusual" words are also less likely to show up in AI-generated writing as they are statistically less common.

* **Letter-like writing (in isolation)**: Although many talk page messages written with salutations, valedictions, subject lines, and other formalities after 2023 tend to appear AI-generated, letters and emails have conventionally been written in such ways *long* before modern LLMs existed.

* **Conjunctions (in isolation)**: While LLMs tend to overuse connecting words and phrases in a stilted, formulaic way, such uses are typical of essay-like writing by humans and are not strong indicators by themselves.

* **Bizarre wikitext/HTML**: While LLMs may hallucinate templates or generate wikitext code with invalid syntax, they are not likely to generate content with certain random-seeming, "inexplicable" errors and artifacts.

## Historical Indicators

The following indicators were common in text generated by older AI models, but are much less frequent in newer models. They may still be useful for finding older undetected AI-generated edits. Dates are approximate.

### Didactic Disclaimers (2022–2024)

**Words to watch:** *it's important/critical/crucial to note/remember/consider*, *worth noting*, *may vary*...

Older LLMs (~2023) often added disclaimers about topics being "important to remember." This frequently took the form of advice to an imagined reader regarding safety or controversial topics.

> **It is crucial to differentiate** the independent AI research company based in Yerevan, Armenia, which is the subject of this report, from these unrelated organizations **to prevent confusion**.

### Section Summaries

**Words to watch:** *In summary*, *In conclusion*, *Overall* ...

When generating longer outputs (such as when told to "write an article"), older LLMs often added sections titled "Conclusion" or similar, and often ended paragraphs or sections by summarizing and restating its core idea.

### Prompt Refusal

**Words to watch:** *as an AI language model*, *as a large language model*, *I cannot offer medical advice, but I can...*, *I'm sorry* ...

In the past, AI chatbots occasionally declined to answer prompts as written, usually with apologies and reminders that they are AI language models. Outright refusals have become increasingly rare.

### Abrupt Cut Offs

AI tools used to abruptly stop generating content if an excessive number of tokens had been used for a single response.

---

*This document is adapted from Wikipedia's "Signs of AI writing" page and is intended as a reference guide for identifying AI-generated content.*
